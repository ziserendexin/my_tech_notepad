  从机器学习的角度讨论如何建一个神圣的卡拉！
——机器学习，机器间的交流建模
把人类互相交流的，转化为机器学习…
假定存在三个学习模型，ABC，学习集合为D，AB学习的集合为D_a,D_b，C学习的为全集C…
如何让AB互相交流，互相给出例子，以改进自己的已学模型，从而尽快达到C的模型…
进阶则是三者的学习方式(改进方式)都不一样，那么如何达到很高的正确率？

PS：背景：假定人是一种很复杂的学习机，有两个部分，一部分是学习方法，对应机器学习的各种方法，另一部分是学习数据，既包括学习数据集及数据集所形成的网络。那么怎么让交流障碍降低呢？

第一层建模：
假定两个学习机的学习方法一样。
模型化一步就是，两个已成学习机(A,B)，但因为精力(现实)问题，只能学习全量数据(D)一部分数据（D_aD_b）。他们之间如何通过交换一些自己已学习的数据（模拟人类交流），来改进自己的模型，从而在判断新数据的时候，结果相差尽量小（达成共识）。
aim暂定为：min 交换数据 + 判断新数据误差较小。

第二层建模：
修改aim，让两者通过交换尽量少的数据，达到最好的效果，也就是与学习了全集的学习机（C学了全集D），的判断结果误差尽量小。
这个是模拟：如何通过机器间有限交流，改进自己的学习方式、学习结构，来达到可以可以对全集最佳的认知。

第三层建模：
一二层的建模基于AB的学习方法一样，但人是不一样的，实际中的学习也是不一样的，如果两者学习方法不同，如何做？

模拟问题及模拟结论：
1、如果学习方法相同，需要交换多少数据（百分比）才能达到（60%、80%）共识？
2、达到的共识，与真实的数据相差多少？
3、如果学习方法不同，能达成共识么？需要交换多少数据？或者甚至需要额外学习更多的数据以证实？
4、如果其中一个机器B，学习的数据不符合随机性，A需要交换多少数据，才能与B达成共识？
5、基于4，A不能与B达成共识的最大问题：猜测在于，A的所有学习的数据(符合随机取样)，都不能扭转B的模型。B的学习方法，对旧数据的占有量过大。

同时，这个也是一种分布式机器学习的一种交互方式？maybe。

伸手求点论文0.0
  