## 界定“学习”

Mitchell (1997) 提供了一个简洁的定义：‘‘对于某类任务T 和性能度量P，一个计算机程序被认为可以从经验E 中学习是指，通过经验E 改进后，它在任务T 上由性能度量P 衡量的性能有所提升。

### 任务T

一般来说，指为机器学习系统应该如何处理**样本**。

常见的任务有:

- 分类

  当前最好的对象识别技术，是基于深度学习的。

  对象识别。

- 输入缺失分类

  当一些输入可能缺失的时候，学习算法必须学习一组函数，而不是当个分类函数。每个函数对应着分类具有不同缺失输入子集$x'​$。

  这个常见于医疗诊断。

  但这个可以简化为，机器只需要学习一个描述联合概率分布的函数即可。

- 回归

  与分类问题有点像。

  预测索赔金额、预测债券等。

- 转录

  观测一些相对非结构化表示的数据，并转录为离散文本的形式。

  比如，根据文本图片，返回ascii码。

  谷歌一这种方式处理街道编号。

  比如，语音识别。

- 机器翻译

  输入一种语音的符号序列，需要将其转化为另一种语音的符号序列。

- 结构化输出

  比如：语法分析，映射自然语言句子到语法结构树，并对节点进行标记。

  比如：图像的像素级分割。

- 异常检测

  信用卡诈骗检测。

  > <https://www.zhihu.com/question/30508773>
  >
  > <https://www.cnblogs.com/bonelee/p/10250096.html>

- 合成与采样

  自动生成相似的样本。

  比如，视频游戏的风景文理。

  比如，语音合成任务中的音频波形。

  这是一类结构化输出任务，只是输出并不只是一个，而是多样化，使得更加自然真实。

- 缺失值填补

- 去噪

- 概率质量函数估计


## 性能度量 P

首先需要明确两个概念：

- 准确率(accuracy):模型输出正确结果的样本比率。
- 错误率(errorate)

## 经验E

- 无监督学习：

  训练含有很多特征的数据集。比如，密度估计；或者是合成、去噪，聚类。

  大致来说，观察随机向量$x$的样本，试图隐式或显式地学习出概率分布，或者其它有意思的性质。

- 监督学习：

  每个都有标签(label)或者目标(target)。

  观察随机向量$x$，及其相关的值或者向量$y$，然后从$x$预测$y$，通常估计是$P(y|x)$。

当然，不要被这个划分给限制住。两者之间是可以互相转换的。

比如：从概率的链式法则表明对于向量$x\in R^n$,学习联合分布$p(x)$，可以拆解为：
$$
p(x)=\prod_{i=1}^{n}p(x_i|x_1,....x_{i-1})
$$
该分解意味着可以将其拆分为n个监督学习问题。

粗略地，将回归、分类、结构化输出问题，称为监督学习。

比如针对线性回归$y=w^{T}x$：
$$
MSE_{test}=\frac{1}{m}\sum_i(\hat{y}^{(test)}_i-y^{(test)}_i)^2
$$
为了构建一个机器学习的算法，需要设计以个算法，通过观察训练集$(X^{(train)},Y^{(train)})$获取经验，减少$MSE_{test}$,以改进权重$w$。

针对线性回归，可知，其最优解为：
$$
\nabla_wMSE_{train}=0\\
\Rightarrow w=(X^{(trian)T}X^{(trian)})^{-1}X^{(trian)T}y^{(train)}
$$

## 容量、过拟合、欠拟合

泛化、训练需查、泛化误差（测试误差）。









