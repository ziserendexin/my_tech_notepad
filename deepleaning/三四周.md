# 前情回顾

逻辑回归的实现形式。

现在使用一种新的表示方法：

![](2-jpg/1.jpg)

使用 [1]、[2]表示深度学习各层的次序。

突然想到之前的习题，通过对实验组的99%拟合，得到了对测试组的67%的结果。从某个角度来说，只比瞎猜强一点，强17%，并且很可能是与图片颜色占比有关。

# 神经网络的分层

input layer：神经网络的输入层

hidden layer：神经网络的隐藏层

output layer：神经网络的输出层

```
a[0] = X	#（输入层）对于下一层的激活值
a[1] = balabala
a[2] = Y_hat
```

神经网络的层数与输入层无关，所以最少一层。（没有隐藏层，只有中间层）

![](2-jpg/2.jpg)

将其向量化。