# 超参数调优

对于众多超参，需要找到一个普遍的方法来进行调优。

比如对2个超参的调优，可以适用5x5的组合中，找到最好的一组。

也可以在这个矩形范围内，随机选择25个。

![](jpg/7.jpg)

这样做的好处是，针对任意一个参数，实际上尝试了25个，而不是5个。

进行粗抽样以后，可以在更小的范围内，进行二次取样。

## 适用合适的尺度

- 对于$\alpha=0.00000001$

  可以在对数尺度上进行均匀随机。

  ![](jpg/8.jpg)

  比如在py中:

  ```python
  r=-4*np.random.rand()	#[-4,0]
  alpha=10^r
  ```

- 指数加权平均$\beta$

  $\beta=0.9........0.9999$

  考虑计算，$1-\beta$，这就和上面的$\alpha$一样了。

  这样计算主要是因为，当$\beta$越靠近1的时候，对于改变就越敏感。

## 对于超参模型的处理方式

- babysitting（panda模式。。。。2333）

  一次运行时间过长，没有训练时间，需要时刻关注模型，使用不同的方式，随时调整参数。

- Caviar模式，鱼子酱模式

  一次运行时间短，有的是手段调参。

具体是哪个，看需求。比如在视觉识别和在线广告，很可能就是babtsitting了。

## 批量归一化

可以让超参变得简单，让神经网络有更好的鲁棒性。

比如对于单层，做如下处理：
$$
\mu=\frac{1}{m}\sum x^{(i)}\\
X=X-\mu\\
\sigma^2=\frac{1}{m}\sum x^{(i)}\\
X=X/\sigma^2
$$
![](jpg/9.jpg)

对任意一层，进行归一化的操作，叫做**batch norm**

假设有一些神经网络的中间值：$z^{(1)}....z^{(m)}$(准确来说是：$z^{[l](i)}​$,第l层，第i个。)

则对这一层，计算平均值、方差。（注意在算方差的时候加上个$\epsilon$）

当然，不是对于所有参数都希望这么干的：
$$
\tilde{z}^{(i)}=\gamma z^{(i)}_{norm}+\beta
$$
$\gamma$和$\beta$可以在训练中得到。

如果，$\gamma=\sqrt{\sigma^2+\epsilon}，\beta=\mu$。那么就相当于消除了归一化。

使用其他的值，可以让这里变成任何参数。

可以设置$\gamma$和$\beta$可以控制$z^{[l][i]}$在任何想要的范围内。

