# 关于训练集train、开发集dev、测试集test

取得参数的最快的方式，毫无疑问是多次进行训练。

数据会被分为三部分，使用开发集优化参数(事前评估)，最后用测试集进行评估。

按照7/3,6/2/2的比例进行。

当然如果数量足够多的话，也不一定按照上面的比例。比如100w/1w/1w。



同时需要考虑到数据集的不同，确保数据集的相似性。

使用爬虫技术，来获取一个足够大的数据集。

## 偏差bias、方差variance

偏差-方差困境。

> http://liuchengxu.org/blog-cn/posts/bias-variance/
>
>  偏差与方差构成了学习器误差的主要成分,偏差小的模型往往方差较大,而方差小的模型往往偏差较大，对模型选择形成两难。

~~即1-范数与2-范数无法兼得？~~

<!--（小孩才做选择，我选择两个都要？归一加参数，优化代价函数！）-->

偏差低=》欠拟合

偏差高=》过拟合



偏差是对训练集而言的。

方差是对测试集而言的。

## 一些比较基本的原则

- 是否有较高的偏差？
  - 如果连training都不能很好的拟合，考虑换个神经网络，或者bigger？
  - 人能否很好地判断？
- 高variance？
  - 更多的数据
  - 尝试正则化
  - 更好的神经网络

- 对于无法同时实现两个bias and variance的争论，这个在机械学习的前期是存在的。
  - 但现在基本上来说，扩大数据，扩大层级，可以两者一同降低。（恰当的正则化）

## 什么是神奇的正则化？

有助于降低过拟合。
$$
aim:\ \ \ \min_{w,b}\ J(w,b)
$$
此时$J(w,b)=\frac{1}{m}\sum\limits_{i=1}^{m}l(\hat{y}^{(i)},y^{(i)})​$。

此时只需要向$J$加入正则化参数即可。
$$
regularization\ = \ \frac{\lambda}{2m}\|w\|^2_2
$$
此时使用的是L-2范数。

以及为啥只对W进行L-2范数，因为在求导的时候，b没啥意义。以及
$$
\|w\|^2\ =\ \sum\limits_{j=1}^{n_x}w_j^2 = w^T w
$$
当然也有对应的L-1正则化：
$$
regularization\ = \ \frac{\lambda}{m}\|w\|_1 
$$
使用L-1，会导致矩阵变得稀疏，变得包含很多0。

> 那么问题来了，L-2范数实际上指代的是啥子玩意？换成矩阵来看，这个实际上是。。F-范数？(2范数是$A^TA$的最大特征值)
>
> 好吧，实际上1、2、3都属于p范数中的一种，
> $$
> \left\|A\right\|_{p}=\max \limits _{{x\neq 0}}{\frac  {\left\|Ax\right\|_{p}}{\left\|x\right\|_{p}}}=\max \limits _{{x\neq 0}}{\frac  {\left(\sum _{{i=1}}^{n}|\sum _{{j=1}}^{m}A_{{ij}}x_{j}|^{p}\right)^{{1/p}}}{\left(\sum _{{i=1}}^{m}|x_{i}|^{p}\right)^{{1/p}}}}
> $$
> 对于1和$\infty$而言，有：
> $$
>  \begin{align}
> & \left \| A \right \| _1 = \max \limits _{1 \leq j \leq n} \sum _{i=1} ^m | a_{ij} | \\
> & \left \| A \right \| _\infty = \max \limits _{1 \leq i \leq m} \sum _{j=1} ^n | a_{ij} | 
> \end{align} 
> $$
> 对于2范数而言：
> $$
> \left \| A \right \| _2=\sqrt{\lambda_{\text{max}}(A^* A)}
> $$
> $\lambda_{\text{max}}$代表最大特征值。
>
> 同时，任意p范数，都小于谱半径：
> $$
> \rho(A) = \max \left \{ |\lambda_1|, \cdots, |\lambda_n| \right \}\\
>  \left \| A \right \| \ge \rho(A)
>  \\可证：\ \ \ \lim _{{r\rightarrow \infty }}\|A^{r}\|^{{1/r}}=\rho (A)
> $$
> 看到这里，问题又来了，为什么(最大)特征值越小越好？
>
> 想想特征值是干嘛的$Ax=\lambda x$,同时任意向量可以表示为不同特征值的和（如果秩比较合适的话）。
>
> 这意思就是可以尽量保证原始向量在经过运算后，保持原有含义？

$\lambda$是正则化参数。是另外一个需要调优的参数。

> PS：python中，lambda是一个保留参数，用于定义一个匿名参数：`g = lambda x:x+1`。简化函数的定义过程。所以用lambd表示，wwww

### **实际上不是**

还是没能深刻理解矩阵的范数含义。

这里应该推导出来的是F-范数：
$$
\|A\|_F=\sqrt{\sum_{i=1}^m\sum_{j=1}^n |a_{ij}|^2}=\sqrt{\operatorname{trace}(A^{{}^*} A)}=\sqrt{\sum_{i=1}^{\min\{m,\,n\}} \sigma_{i}^2}
$$
这里$A^*$表示*A*的[共轭转置](https://zh.wikipedia.org/wiki/%E5%85%B1%E8%BD%AD%E8%BD%AC%E7%BD%AE)，$σ_i$是*A*的[奇异值](https://zh.wikipedia.org/wiki/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3)，并使用了[迹函数](https://zh.wikipedia.org/wiki/%E8%BF%B9)。

<!--矩阵中所有元素的平方和。-->

### **如何计算偏导？**

$$
\begin{align}
dw^{[l]}&=\ (原来那块)+\frac{\lambda}{m}w^{[l]}\\
&=\ (1-\frac{\alpha\lambda}{m})(原来那块)
\end{align}
$$

所以这里也被称为**权重削减**方法。

# why？

当$\lambda$过大的时候，$w$会变小，也就是说$z$会比较小(趋于0)，那么对于$tanh(z)$的激活函数，会集中于中间的线性部分。那么就是说，失去了深度学习的非线性的优点。这样就会造成欠拟合以及高方差。

当然，这样也不容易过拟合了。

### 非线性越强，越容易过拟合？

总感觉怪怪的。

> https://blog.csdn.net/u012162613/article/details/44261657
>
> 这里就是尽量约束拟合函数在某些小区间的绝对值范围。

<!--虽然这只是一种象形的解释，不是足够的严谨，但这里也够用了-->

<!--深度学习依然处于炼蛊与造蛊之间，很多东西都是，用着好使，但为啥好使。。。emmm，得去看论文啊？-->

# dropout正则化

对所有节点中，随机地丢弃掉一部分节点。

比如：

![](jpg/1.jpg)

虽然看起来很粗暴，但。。。挺好用的。

### dropout技术：

```python
d3 = np.randm.rand(a3.shape[0],a3.shape[1]) < keep_prob	# keep_prob 保留概率
a3 *= d3
a3 /= keep_pron	# 保持期望值不变
z4 = w4 · a3 + b
z4 /= keep_pron
```

这里说，可以在不同次的循环中，给不同的节点进行失活。emmm。。。

# why？part2

这个的why倒是比较简单，降低w的改变频率，使其更加平缓一些。

> 更加深入地理解，可以看看Hinton和Alex两牛2012的论文《ImageNet Classification with Deep Convolutional Neural Networks》
>
> <!--泥奏凯-->

> https://blog.csdn.net/u012162613/article/details/44261657 
>
> 可以简单地这样解释，运用了dropout的训练过程，相当于训练了很多个只有半数隐层单元的神经网络（后面简称为“半数网络”），每一个这样的半数网络，都可以给出一个分类结果，这些结果有的是正确的，有的是错误的。随着训练的进行，大部分半数网络都可以给出正确的分类结果，那么少数的错误分类结果就不会对最终结果造成大的影响。

一开始是使用在视觉处理中，反正挺好使的。因为数据集一般都会很大很大。

但在其他领域，除非以及过拟合，不然不太会使用这个技术。

当然使用这玩意的时候，J的值就会变得不明确了，不能用来debug了。这时候可以考虑把drop-out给关掉。（为什么不再保存一个没有drop-out的J？虽然也不是单调下降的）