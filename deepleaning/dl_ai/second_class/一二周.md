# 关于训练集train、开发集dev、测试集test

取得参数的最快的方式，毫无疑问是多次进行训练。

数据会被分为三部分，使用开发集优化参数(事前评估)，最后用测试集进行评估。

按照7/3,6/2/2的比例进行。

当然如果数量足够多的话，也不一定按照上面的比例。比如100w/1w/1w。



同时需要考虑到数据集的不同，确保数据集的相似性。

使用爬虫技术，来获取一个足够大的数据集。

## 偏差bias、方差variance

- 0<error(train)<<error(dev) ：high variance
- 0<<error(train)≈error(dev) ：high bias
- 0<<error(train)<<error(dev)：high bias & high variance
- 0<error(train)≈error(dev)：low bias & low variance

这里需要考虑贝叶斯error，如果对于人类的判断也不高，那么可能第二个的分类器效果就是不错的。这里需要出去这个影响。(人类的判断)

偏差-方差困境。

> http://liuchengxu.org/blog-cn/posts/bias-variance/
>
>  偏差与方差构成了学习器误差的主要成分,偏差小的模型往往方差较大,而方差小的模型往往偏差较大，对模型选择形成两难。

~~即1-范数与2-范数无法兼得？~~

<!--（小孩才做选择，我选择两个都要？归一加参数，优化代价函数！）-->

偏差低=》欠拟合

偏差高=》过拟合



偏差是对训练集而言的。

方差是对测试集而言的。

当然还需要考虑到在训练集和测试集之间的准确。

## 一些比较基本的原则

- 是否有较高的偏差？
  - 如果连training都不能很好的拟合

    考虑换个神经网络，

    - 更多的隐藏层
    - 更多的隐藏单元
    - 延长训练时间

  - 更高级的优化算法

  - (更适合的深度学习结构)

    与下面的那个相同

  - 人能否很好地判断？
- 高variance？
  - 更多的数据

  - 尝试正则化

  - 更好的神经网络（这里的更好指得更适合的深度学习的结构）

    可能会运用到更多的方法，比如卷积神经网络啊等等，但也不一定好用。

- 对于无法同时实现两个bias and variance的争论，这个在机械学习的前期是存在的。

  - 但现在基本上来说，扩大数据，扩大层级，可以两者一同降低。（恰当的正则化）

## 什么是神奇的正则化？

有助于降低过拟合。
$$
aim:\ \ \ \min_{w,b}\ J(w,b)
$$
此时$J(w,b)=\frac{1}{m}\sum\limits_{i=1}^{m}l(\hat{y}^{(i)},y^{(i)})​$。

此时只需要向$J​$加入正则化参数即可。
$$
regularization\ = \ \frac{\lambda}{2m}\|w\|^2_2
$$
此时使用的是L-2范数。

以及为啥只对W进行L-2范数，因为在求导的时候，b没啥意义。以及
$$
\|w\|^2\ =\ \sum\limits_{j=1}^{n_x}w_j^2 = w^T w
$$
当然也有对应的L-1正则化：
$$
regularization\ = \ \frac{\lambda}{m}\|w\|_1 
$$
使用L-1，会导致矩阵变得稀疏，变得包含很多0。

> 那么问题来了，L-2范数实际上指代的是啥子玩意？换成矩阵来看，这个实际上是。。F-范数？(2范数是$A^TA$的最大特征值)
>
> 好吧，实际上1、2、3都属于p范数中的一种，
> $$
> \left\|A\right\|_{p}=\max \limits _{{x\neq 0}}{\frac  {\left\|Ax\right\|_{p}}{\left\|x\right\|_{p}}}=\max \limits _{{x\neq 0}}{\frac  {\left(\sum _{{i=1}}^{n}|\sum _{{j=1}}^{m}A_{{ij}}x_{j}|^{p}\right)^{{1/p}}}{\left(\sum _{{i=1}}^{m}|x_{i}|^{p}\right)^{{1/p}}}}
> $$
> 对于1和$\infty$而言，有：
> $$
>  \begin{align}
> & \left \| A \right \| _1 = \max \limits _{1 \leq j \leq n} \sum _{i=1} ^m | a_{ij} | \\
> & \left \| A \right \| _\infty = \max \limits _{1 \leq i \leq m} \sum _{j=1} ^n | a_{ij} | 
> \end{align} 
> $$
> 对于2范数而言：
> $$
> \left \| A \right \| _2=\sqrt{\lambda_{\text{max}}(A^* A)}
> $$
> $\lambda_{\text{max}}$代表最大特征值。
>
> 同时，任意p范数，都小于谱半径：
> $$
> \rho(A) = \max \left \{ |\lambda_1|, \cdots, |\lambda_n| \right \}\\
>  \left \| A \right \| \ge \rho(A)
>  \\可证：\ \ \ \lim _{{r\rightarrow \infty }}\|A^{r}\|^{{1/r}}=\rho (A)
> $$
> 看到这里，问题又来了，为什么(最大)特征值越小越好？
>
> 想想特征值是干嘛的$Ax=\lambda x$,同时任意向量可以表示为不同特征值的和（如果秩比较合适的话）。
>
> 这意思就是可以尽量保证原始向量在经过运算后，保持原有含义？

$\lambda$是正则化参数。是另外一个需要调优的参数。

> PS：python中，lambda是一个保留参数，用于定义一个匿名参数：`g = lambda x:x+1`。简化函数的定义过程。所以用lambd表示，wwww

### **实际上不是**

还是没能深刻理解矩阵的范数含义。

这里应该推导出来的是F-范数：
$$
\|A\|_F=\sqrt{\sum_{i=1}^m\sum_{j=1}^n |a_{ij}|^2}=\sqrt{\operatorname{trace}(A^{{}^*} A)}=\sqrt{\sum_{i=1}^{\min\{m,\,n\}} \sigma_{i}^2}
$$
这里$A^*$表示*A*的[共轭转置](https://zh.wikipedia.org/wiki/%E5%85%B1%E8%BD%AD%E8%BD%AC%E7%BD%AE)，$σ_i$是*A*的[奇异值](https://zh.wikipedia.org/wiki/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3)，并使用了[迹函数](https://zh.wikipedia.org/wiki/%E8%BF%B9)。

<!--矩阵中所有元素的平方和。-->

### **如何计算偏导？**

$$
\begin{align}
dw^{[l]}&=\ (原来那块)+\frac{\lambda}{m}w^{[l]}\\
&=\ (1-\frac{\alpha\lambda}{m})(原来那块)
\end{align}
$$

所以这里也被称为**权重削减**方法。

# why？

当$\lambda$过大的时候，$w$会变小，也就是说$z$会比较小(趋于0)，那么对于$tanh(z)$的激活函数，会集中于中间的线性部分。那么就是说，失去了深度学习的非线性的优点。这样就会造成欠拟合以及高方差。

当然，这样也不容易过拟合了。

### 非线性越强，越容易过拟合？

总感觉怪怪的。

> https://blog.csdn.net/u012162613/article/details/44261657
>
> 这里就是尽量约束拟合函数在某些小区间的绝对值范围。

<!--虽然这只是一种象形的解释，不是足够的严谨，但这里也够用了-->

<!--深度学习依然处于炼蛊与造蛊之间，很多东西都是，用着好使，但为啥好使。。。emmm，得去看论文啊？-->

# dropout正则化

对所有节点中，随机地丢弃掉一部分节点。

比如：

![](jpg/1.jpg)

虽然看起来很粗暴，但。。。挺好用的。

### dropout技术：

```python
d3 = np.randm.rand(a3.shape[0],a3.shape[1]) < keep_prob	# keep_prob 保留概率
a3 *= d3
a3 /= keep_pron	# 保持期望值不变
z4 = w4 · a3 + b
z4 /= keep_pron
```

这里说，可以在不同次的循环中，给不同的节点进行失活。emmm。。。

# why？part2

这个的why倒是比较简单，降低w的改变频率，使其更加平缓一些。

> 更加深入地理解，可以看看Hinton和Alex两牛2012的论文《ImageNet Classification with Deep Convolutional Neural Networks》
>
> <!--泥奏凯-->

> https://blog.csdn.net/u012162613/article/details/44261657 
>
> 可以简单地这样解释，运用了dropout的训练过程，相当于训练了很多个只有半数隐层单元的神经网络（后面简称为“半数网络”），每一个这样的半数网络，都可以给出一个分类结果，这些结果有的是正确的，有的是错误的。随着训练的进行，大部分半数网络都可以给出正确的分类结果，那么少数的错误分类结果就不会对最终结果造成大的影响。

一开始是使用在视觉处理中，反正挺好使的。因为数据集一般都会很大很大。

但在其他领域，除非以及过拟合，不然不太会使用这个技术。

当然使用这玩意的时候，J的值就会变得不明确了，不能用来debug了。这时候可以考虑把drop-out给关掉。（为什么不再保存一个没有drop-out的J？虽然也不是单调下降的）

# 其他的方法

1. 可以使用旋转等等的方法，增加图片，增加数据。0.0

   这也行？

   实际上是给模型增加一层旋转、对称、变形等操作。告诉它，旋转以后，还是猫。

   这也从某种程度上减少了模型的过拟合概率。

2. 提前停止迭代过程。

   在训练集和测试集交叉表现最好的时候，停止。（？也就是说吧测试集整理一部分进去？）

当然，在算力足够的情况下，L2正则化还是很好的。

## 对输入进行归一化

因为梯度下降程度$\alpha$是固定的，所以如果不适用归一化，很容易浪费计算量的下降的过程中，而归一化，就是减少中间的这个浪费。让代价函数更圆。www

![](jpg/2.jpg)

具体的做法是：

令其均值为0，方差为1:
$$
x\ -=\ \mu \\
x\ /=\ \sigma^2
$$

# 梯度消失与爆炸

在某些情况下，梯度会降温减少为0或者变得很大。

在很深的网络中，如果$W^{[i]}$ 都$\ >or<\ E$，那么会造成其代价函数变得很大或者很小。如果激活函数同样对其很敏感的话，会让值变得非常奇怪。这时候训练起来就会变得非常困难。

尤其是初始化的时候，如果有这样的情况，训练的热启动就会变得很困难。

这个大概率出现在初始化以后。

## how to deal it？  更细致地初始化W

![](jpg/3.jpg)

对于一个单层的网络，有：
$$
z\ =\ w_1x_1+w_2x_2+w_3x_3+...+w_nx_n
$$
我们希望z不会过大过小，那么也就是说希望w可以随着n的增加尽可能地小。

即：$Var(w_i)=\frac{1}{n}​$。
$$
w^{[l]}=np.random.ran(shape)*np.sqrt(\frac{1}{n^{[l]}-1})
$$
如果是Relu函数，这里是期望，$Var(w_i)=\frac{2}{n}$。所以记得乘个2。

对于各种激活函数，有：

Relu：$\sqrt{\frac{2}{{n^{[l]}-1}}}$。

tanh:$\sqrt{\frac{1}{{n^{[l]}}}}​$。

更一般的：$\sqrt{\frac{2}{{n^{[l-1]}+n^{[l]}}}}​$。

# 梯度检验

保证梯度逆传播的实现是正确的。

通过$\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}​$，的方式，求梯度。

这个叫啥来着？一阶中心差值。

具体到这里就是，记住$J[i-1]、J[i]、J[i+1]$,三个来计算下？0.0？

**这玩意可以帮你检验出逆传播中的错误。是一个非常优秀的debug方法**

首先，将$W^{[1]},b^{[1]},...，W^{[L]},b^{[L]}$，组合成一个很大的向量$\theta$。

将W矩阵、b转换为向量，再将其所有连起来。

同样处理$dW^{[1]},db^{[1]},...，dW^{[L]},db^{[L]}​$，组合成很大的向量$d\theta​$。

对于$d\theta_{appear}[i]​$,有：
$$
d\theta_{appear}[i]\ =\ \frac{J(\theta_1,..\theta_i+\epsilon)-J(\theta_1,...,\theta_i-\epsilon)}{2\epsilon}
$$
此时，这个的值应该近似于
$$
d\theta_{appear}[i]\ \approx \frac{dJ}{d\theta_i}
$$
此时有两个值，$d\theta_{appear}​$和$d\theta​$。

如何判断两者大致相等呢，
$$
\frac{\|d\theta_{appear}-d\theta\|_2}{\|d\theta_{appear}\|_2+\|d\theta\|_2}\ \approx \ \epsilon
$$
如果相差的数量级过大，那么就需要检查了。

- 不要在训练中使用，只在debug中使用

  计算$d\theta_{appear}[i]$的速度是非常慢的。

- 如果相差很大，那么逐$i​$检查，会很有帮助。

- 注意加上正则化项

  $d\theta \ = \ grand\ J+正则化项​$

- 随机失活的话，就没有一个很客观的J了，基本没办法运行

  所以注意将`keep_prob`设定为1。

- 初始的时候，可能因为w、b的初始值，导致很不好。

  先运行几轮，在进行检查。

  

## 测试：

嘿，为啥啊？

开发集合测试集：

来自于不同的 distribution，分布是错的。。。额，如果这里理解为分布，那么应该选A。

高偏差只选，正则与训练集。

增加保留率：

降低正则影响、造成低训练集错误。