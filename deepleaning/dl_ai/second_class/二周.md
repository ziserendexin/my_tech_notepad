虽然这是一个很好的算法，但运算速度还是不够快。

## 小批量梯度下降

矢量化运算，将分开的向量，合并到一个超大的矩阵。这样在一定程度上可以加快速度。

但如果整个m过大的话，那么每次运算就会变得很慢很慢，那么是不是有一些在运算完整个模型以前就可以有那么一些成效的方法呢？



$m=5,000,000$

先将训练集拆分为小的训练集：称为(mini-batch)，比如一个微训练集m为1000。

那么
$$
X=[X^{[1]}X^{[2]}...X^{[t=5000]}]
$$
看起来这个过程并不怎么难理解。

![](jpg/6.jpg)

**这么来说，可以通过小批量梯度下降，来进行任务分发！并行！**

但这个只是正向的。。。哦，逆向好像不太需要那么多运算量哦。

> **那么问题来了，使用小批量梯度下降，是否会与直接运行整个有所不同？**
>
> 并不会，只是计算J，这还是一个原子操作。

## 深入理解这玩意

mini-batch的大小。

但其等于m的时候，就相当于对整个数据集进行计算，批量梯度下降法。

这玩意有点事效果好，但太大的话也会有问题。

但如果等于1的时候，那就好玩了，这时候会得到一个叫做随机梯度下降的算法。正如其名字一样，每次下降都是一个不可预测的随机行为。

**于是这个算法，没计算完一个mini-batch后都进行一次下降啊。0.0**

注意，mini-batch的结果需要可以放进你的CPU/GPU的内存里面。

## 指数加权平均(指数加权滑动平均)

<!--算是一种预处理？-->

比如计算温度趋势的时候，使用$V_i=0.9*V_{i-1}+0.1*\theta_i$，这样就能得到一个比较好的曲线喽。

当然，随着比率$\beta$的增大，曲线会向右移动，并且会变得更加的平滑，新的数据对其的影响会变得很小。

将这个式子展开以后会得到：
$$
V_m=0.1*\sum^{m}_{i=0}0.9^i\theta_i
$$
那么对于之前的数据影响，就是一个指数下降函数。

回忆下关于e的定义式子
$$
e^x = \lim_{n\rightarrow \infty}(1+\frac{x}{n})^n
$$
这里取，$x=-1$。

<!--这玩意扩展下就是卷积？-->

### 偏差修正

对于此函数的起始信息，会比较低的问题，所以需要对此进行修正。

对其做一个参数的修正$\frac{V_t}{1-\beta^t}$，这样就能得到一个正确的平均值了。

## 动量(momentum)梯度下降

emm,也就是对每次的梯度值做一个指数加权平均？

这样可以避免太大的震荡，也就变现地加快了迭代的速度。

$V_{d w}=\beta V_{d w}+(1-\beta )d w$。

可以想象成给梯度下降增加了一个加速度？哈哈哈，有意思的解释。

这里一般去0.9。

同时对于偏差修正，实际上就是对$\alpha$，($W=W-\alpha V_{dW}$)，的修正，实际上没啥区别。因为本来就是个需要调整的超参。

## RMSprop（root mean square prop均方根传递）

目的是减少对参数$b$的修正。引入以下公式,这里的平方是逐方向平方。
$$
S_{dw}=\beta S_{dw}+(1-\beta)dw^2 \\
S_{db}=\beta S_{db}+(1-\beta)db^2 \\
w:=w-\alpha \frac{dw}{\sqrt{S_{dw}+\epsilon}}\\
b:=b-\alpha \frac{db}{\sqrt{S_{db}}}
$$
同时为了防止除0的问题，因此在有一个小的epsilon。

