[TOC]

# 中文文档的学习

> https://www.kubernetes.org.cn/k8s

### k8s的目标

Kubernetes一个核心的特点就是能够自主的管理容器来保证云平台中的容器按照用户的期望状态运行着。

现在Kubernetes着重于不间断的服务状态（比如web服务器或者缓存服务器）和原生云平台应用（Nosql）,在不久的将来会支持各种生产云平台中的各种服务，例如，分批，工作流，以及传统数据库。

### Kubernetes-核心资源之Pod

#### 概述

https://www.kubernetes.org.cn/3965.html

首先，其包括了dock-compose的功能，包括容器网络交互的定义等信息。

并且还额外的增加了更详细的状态检测机制（liveness和readiness，可以通过tcp、http来检测是否正常）、镜像拉取策略、其他pod重启策略、资源限制策略(包括异常了怎么重启)

#### 目标

pod的目标是协助k8s保证某服务，一直启动。

#### 网络

Kubernetes支持一种特殊的网络模型，Kubernetes创建了一个地址空间，并且不动态的分配端口，它可以允许用户选择任何想使用的端口，为了实现这个功能，它为每个Pod分配IP地址。

#### 构架

![architecture](jpg\architecture.png)

Kubernetes主要由以下几个核心组件组成：

- etcd保存了整个集群的状态；
- apiserver提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制；
- controller manager负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；
- scheduler负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上；
- kubelet负责维护容器的生命周期，同时也负责Volume（CVI）和网络（CNI）的管理；
- Container runtime负责镜像管理以及Pod和容器的真正运行（CRI）；
- kube-proxy负责为Service提供cluster内部的服务发现和负载均衡；

除了核心组件，还有一些推荐的Add-ons：

- kube-dns负责为整个集群提供DNS服务
- Ingress Controller为服务提供外网入口
- Heapster提供资源监控
- Dashboard提供GUI
- Federation提供跨可用区的集群
- Fluentd-elasticsearch提供集群日志采集、存储与查询

#### API设计原则

1. **所有API应该是声明式的**。

   正如前文所说，声明式的操作，相对于命令式操作，对于重复操作的效果是稳定的，这对于容易出现数据丢失或重复的分布式环境来说是很重要的。另外，声明式操作更容易被用户使用，可以使系统向用户隐藏实现的细节，隐藏实现的细节的同时，也就保留了系统未来持续优化的可能性。此外，声明式的API，同时隐含了所有的API对象都是名词性质的，例如Service、Volume这些API都是名词，这些名词描述了用户所期望得到的一个目标分布式对象。

2. **API对象是彼此互补而且可组合的**。

3. **高层API以操作意图为基础设计**。

4. **低层API根据高层API的控制需要设计**。

5. **尽量避免简单封装，不要有在外部API无法显式知道的内部隐藏的机制**。

6. **API操作复杂度与对象数量成正比**。

7. **API对象状态不能依赖于网络连接状态**。

8. **尽量避免让操作机制依赖于全局状态，因为在分布式系统中要保证全局状态的同步是非常困难的**。

# 云原生的设计哲学

<https://jimmysong.io/kubernetes-handbook/cloud-native/cloud-native-philosophy.html>

云原生系统的设计理念如下:

- 面向分布式设计（Distribution）：容器、微服务、API 驱动的开发；
- 面向配置设计（Configuration）：一个镜像，多个环境配置；
- 面向韧性设计（Resistancy）：故障容忍和自愈；
- 面向弹性设计（Elasticity）：弹性扩展和对环境变化（负载）做出响应；
- 面向交付设计（Delivery）：自动拉起，缩短交付时间；
- 面向性能设计（Performance）：响应式，并发和资源高效利用；
- 面向自动化设计（Automation）：自动化的 DevOps；
- 面向诊断性设计（Diagnosability）：集群级别的日志、metric 和追踪；
- 面向安全性设计（Security）：安全端点、API Gateway、端到端加密；

## 实验

依然是在centos7的虚拟机上搞搞事。试试这个minikube。

   > 官方文档：https://kubernetes.io/docs/setup/minikube/
   >
   > 中文文档：https://linux.cn/article-8847-1.html
   >

   1. 安装

      对于 Linux，安装 [VirtualBox](https://www.virtualbox.org/wiki/Downloads) 或者 [KVM](http://www.linux-kvm.org/)，然后再安装 Kubectl 和 Minkube。

      > KVM：使用KVM，可以运行多个运行未修改的Linux或Windows映像的虚拟机。每个虚拟机都有专用的虚拟化硬件：网卡，磁盘，图形适配器等。

      > VirtualBox 是一款开源虚拟机软件。使用者可以在VirtualBox上安装并且执行Solaris、Windows、DOS、Linux、[OS/2](https://www.baidu.com/s?wd=OS%2F2&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao) Warp、BSD等系统作为客户端[操作系统](https://www.baidu.com/s?wd=%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)。

      两者选择：因此使用KVM，并且他也是开源的

      > https://blog.csdn.net/wj_j2ee/article/details/7847836
      >
      > vbox 是由 qemu 改写而成，包含大量 qemu 代码。可以使用于 不支持 虚拟化的cpu。值得说的一点：vbox 在图形方面比较好，能进行2D 3D加速。cpu控制不理想（估计是因为图形支持的缘故）。操作上有独立的图形界面，易于上手。
      > kvm 是linux内核包含的东西，使用qemu作为上层管理（命令行）。cpu 必须支持虚拟化。性能，作为服务器很好，可是图形能力十分的差。即使放电影，图像也是像刷油漆一样，一层一层的。cpu使用率控制很好。 控制上比较简洁，功能比较丰富：比如使用 “无敌功能”所有更改指向内存，你的镜像永远保持干净。 “母镜像”功能让你拥有n个独立快照点。 还有很多参数。另外，kvm作为内核级的虚拟机，刚开始发展关注的公司比较多——但是还没有达到商业应用的水平。
      >
      > 总体而言：在支持 虚拟化的情况下，vbox 和 kvm 的性能差不多，主要是面向对象不同：kvm使用于服务器，vbox使用于桌面应用。

   2. 

## 另外一个玩意

https://kubeedge.io/zh/

## 二次学习

> Pod是所有业务类型的基础，它是一个或多个容器的组合。
>
> **这些容器共享存储、网络和命名空间，以及如何运行的规范。**

这句话有点意思，说明这一个套件内的东西，都可以互相访问。

那么同一个pod的东西，能否分配给不同的node？嗯，不行

> <https://kubernetes.io/docs/tutorials/kubernetes-basics/explore/explore-intro/>
>
> A Pod always runs on a **Node**.

建议是从minikube开始。

下载：

```
curl -x 192.168.253.1:1080  -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64   && chmod +x minikube
```

这时候有个文件

## minikube test开始

### 虚拟化驱动

先需要安装虚拟化驱动：VirtualBox或者kvm2。

这里[安装kvm2](<https://github.com/kubernetes/minikube/blob/master/docs/drivers.md#kvm2-driver>)：

**注意：**kvm和kvm2不是一个玩意。

```
curl -x 192.168.253.1:1080 -LO https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-kvm2 && sudo install docker-machine-driver-kvm2 /usr/local/bin/
```

对于VMware，这里有另外的[方案](https://github.com/kubernetes/minikube/blob/master/docs/drivers.md#vmware-unified-driver)：

```sh
export LATEST_VERSION=$(curl -L -s -H 'Accept: application/json' https://github.com/machine-drivers/docker-machine-driver-vmware/releases/latest | sed -e 's/.*"tag_name":"\([^"]*\)".*/\1/') \
&& curl -L -o docker-machine-driver-vmware https://github.com/machine-drivers/docker-machine-driver-vmware/releases/download/$LATEST_VERSION/docker-machine-driver-vmware_darwin_amd64 \
&& chmod +x docker-machine-driver-vmware \
&& mv docker-machine-driver-vmware /usr/local/bin/
```

### kubectl的安装

<https://kubernetes.io/docs/tasks/tools/install-kubectl/>

```sh
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl
```

or

```
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
yum install -y kubectl
```

kubectl 是 Kubernetes 的命令行工具（CLI），是 Kubernetes 用户和管理员必备的管理工具。

<!--minikube与kubectl的关系类似于docker与docker-compose-->

**注意：**需要先安装kubectl，再安装minikube，不然会发生`Unable to connect to the server: unexpected EOF`的问题，根据 [link](https://stackoverflow.com/questions/48928330/how-to-fix-issue-of-unable-to-connect-to-the-server-eof-kubernetes-kubectl) 的描述，可能是因为需要再minikube中队kubectl进行配置？

## minikube

### [设置代理](https://github.com/kubernetes/minikube/blob/master/docs/http_proxy.md)：

最好下载阿里云版的`minikube`，因为内置将代理换成了国内的，如果用阿里云版的，可先跳过代理部分

```
curl -Lo minikube http://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v1.1.0/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/    
```

然后直接即可：（会使用环境变量来设置得代理）

```sh
minikube start
```

```
minikube start --vm-driver vmware --registry-mirror=https://registry.docker-cn.com
```

<!--这里总是不成功，可能是权限还是啥？后续研究吧-->

```
minikube start --vm-driver none --registry-mirror=https://registry.docker-cn.com
```

先尝试使用none来搞搞看看？

好吧，minikube需要2个cpu，2048内存。

启动的时候声明vm-driver为vmware

```sh
minikube start --vm-driver vmware
```

` --alsologtostderr -v=8`为打印详细日志的参数。

```
export HTTP_PROXY="http://192.168.253.1:1080"
export HTTPS_PROXY="https://192.168.253.1:1080"
export NO_PROXY=localhost,127.0.0.1,10.96.0.0/12,192.168.99.0/24,192.168.39.0/24
```

```sh
 --docker-env HTTP_PROXY=$HTTP_PROXY --docker-env HTTPS_PROXY $HTTPS_PROXY --docker-env NO_PROXY=$NO_PROXY
```

以上参数为docker的代理设置

PS：

如果之前有设置过替他驱动(可能没有成功)，则需要删除之前的驱动：

```sh
minikube delete
```



### 启动

```sh
minikube start --vm-driver=kvm2 --docker-env HTTP_PROXY=$HTTP_PROXY --docker-env HTTPS_PROXY $HTTPS_PROXY --docker-env NO_PROXY=$NO_PROXY
```

这个是--docker是设置minikube内部

设置代理

- 1 用provixy将socks代理转为http代理[参考](https://docs.lvrui.io/2016/12/12/Linux中使用ShadowSocks-Privoxy代理/)
- 2 设置代理

```shell
minikube start --vm-driver=kvm2 --registry-mirror=https://registry.docker-cn.com  //声明镜像仓库
minikube start --vm-driver=kvm2 --docker-env HTTP_PROXY=$HTTP_PROXY --docker-env HTTPS_PROXY $HTTPS_PROXY --docker-env NO_PROXY=$NO_PROXY// 声明代理
```

<https://blog.zhoulouzi.com/2017/10/minikube/>



## minikube建立失败

再来，minikube实际上是为了在单机中建立一个集群，然后使用kubectl进行通讯、建立。等操作。

一个pod是一组应用的最小集合。可以有ABC三个容器。

## 先在模拟上看看吧

<https://kubernetes.io/docs/tutorials/kubernetes-basics/explore/explore-interactive/>

使用`kubectl get pods`查看已有节点。通过`kubectl describe pods`查看pods的具体状态。

可以看到名为`kubernetes-bootcamp`的pod中，有一个`kubernetes-bootcamp`的镜像在运行，其IP为172.18.0.4网段等到。

<!--注意，在设计中，这个输出是为了给人看的，不是给脚本用的，脚本用的应该有另外的接口。-->

kubectl 也可以展示pod的环境变量：`kubectl exec $POD_NAME env`，或者进入pod的bash`kubectl exec -ti $POD_NAME bash`,也有显示日志的命令`kubectl logs $POD_NAME`。

比如：

```
kubectl exec $POD_NAME env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=kubernetes-bootcamp-5c69669756-qdf2f
KUBERNETES_SERVICE_HOST=10.96.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
NPM_CONFIG_LOGLEVEL=info
NODE_VERSION=6.3.1
HOME=/root
```

需要注意的是pod是会死亡的，可以使用`ReplicaSet`定义保障性服务，在任何给定时间维护一组稳定的副本Pod。以保证pod的可用性。

有几种ip暴露方式：

- *ClusterIP*

  在群集中的内部IP上公开服务。此类型使服务只能从群集中访问。

- *NodePort*

  使用NAT在集群中每个选定节点的同一端口上公开服务。使用可从群集外部访问服务`<NodeIP>:<NodePort>`。ClusterIP的超集。

  <!--这个是在一个确定的node上暴露，-->

- *LoadBalancer*

  在当前云中创建外部负载均衡器（如果支持），并为服务分配固定的外部IP。NodePort的超集。

- *ExternalName*

  `externalName`通过返回带有名称的CNAME记录，使用任意名称（在规范中指定）公开服务。没有代理使用。此类型需要v1.7或更高版本`kube-dns`。

  <!--这个有点类似docker中的alais-->

当然这里建议的方式是，使用services进行pod服务的暴露。[link](https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/)

## 拓展应用

比如将一个service的一个pod拓展为两个pod。

```sh
$ kubectl get deployments
NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-bootcamp   1         1         1            0           8s
```

- DESIRED：配置的副本数
- CURRENT：当前正在运行的副本数
- UP-TO-DATE：已更新到设定的副本数
- AVAILABLE：可用的

使用如下命令即可对其进行扩容操作。

```
kubectl scale deployments/kubernetes-bootcamp --replicas=4
```

扩容完以后，可以检查pod，发现的确变多了。IP也不一样。

然后是最关心的流量负载问题：

1. 做一个node_prot接口
2. 然后就可以从minikube的集群接口获取数据了。

## 滚动更新

<https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/>

<!--我擦！黑科技啊！-->

用户希望应用程序始终可用，开发人员应该每天多次部署新版本的应用程序。在Kubernetes中，这是通过滚动更新完成的。**滚动更新**允许通过使用新的实例逐步更新Pods实例来实现部署的更新，从而实现零停机。新的Pod将在具有可用资源的节点上进行调度。

```
kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2
```

只需要如此这样一条命令，即可完成升级操作。

如果升级错了，可以通过：

```
kubectl rollout undo deployments/kubernetes-bootcamp
```

来取消升级

## 虚拟机安装

一个step by step的项目<https://github.com/opsnull/follow-me-install-kubernetes-cluster>

中文官方：<https://kubernetes.io/zh/docs/concepts/>

> 首先常规安装docker：
>
> ```sh
> yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
> systemctl restart packagekit
> yum makecache fast
> yum -y install yum-utils device-mapper-persistent-data lvm2
> yum -y install docker-ce
> systemctl enable docker
> systemctl start docker
> ```
>
> 然后是安装kubernetes包：
>
> 这里需要禁用selinux：
>
> > Now time to install Kubernetes packages, we need yum repo from google Also disable selinux as docker uses cgroups and other lib which selinux falsely treats as threat.

# kubernetes宏观研究

Kubernetes 控制面由一组运行在集群上的进程组成：

- **Kubernetes 主控组件（Master）** 包含三个进程，都运行在集群中的某个节上，通常这个节点被称为 master 节点。这些进程包括：[kube-apiserver](https://kubernetes.io/docs/admin/kube-apiserver/)、[kube-controller-manager](https://kubernetes.io/docs/admin/kube-controller-manager/)和[kube-scheduler](https://kubernetes.io/docs/admin/kube-scheduler/)。
- 集群中的每个非 master 节点都运行两个进程：
  - **kubelet**，和 master 节点进行通信。
  - **kube-proxy**，一种网络代理，将 Kubernetes 的网络服务代理到每个节点上。





# [follow-me-install-kubernetes-cluster](https://github.com/opsnull/follow-me-install-kubernetes-cluster)项目实践

### 主机配置

- 主机名

本地的ip:

```
192.168.253.140 k8s_1
192.168.253.139 k8s_2
192.168.253.141 k8s_3
```

设置hostname：

```
hostnamectl set-hostname k8s_1
hostnamectl set-hostname k8s_2
hostnamectl set-hostname k8s_3
```

```
cat >> /etc/hosts <<EOF
192.168.253.140 k8s_1
192.168.253.139 k8s_2
192.168.253.141 k8s_3
EOF
```

```
ssh-keygen -t rsa
ssh-copy-id root@k8s_1
ssh-copy-id root@k8s_2
ssh-copy-id root@k8s_3
```

- 无密码 ssh 登录其它节点

- 更新 PATH 变量

- 安装依赖包

- 关闭防火墙

- 关闭 swap 分区

- 关闭 SELinux

- 加载内核模块

- 优化内核参数

- 设置系统时区

- 关闭无关的服务

- 设置 rsyslogd 和 systemd journald

- 创建相关目录

- 升级内核【放弃该操作】

- NUMA【放弃该操作】

- 分发集群配置参数脚本【实际上这步超级重要】：

  > 环境变量都定义在文件 [environment.sh](https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/manifests/environment.sh) 中，请根据**自己的机器、网络情况**修改。然后，把它拷贝到**所有**节点的 `/opt/k8s/bin` 目录：
  >
  > ```
  > source environment.sh
  > for node_ip in ${NODE_IPS[@]}
  >   do
  >     echo ">>> ${node_ip}"
  >     scp environment.sh root@${node_ip}:/opt/k8s/bin/
  >     ssh root@${node_ip} "chmod +x /opt/k8s/bin/*"
  >   done
  > ```

### 证书生成

- cfssl工具集：

  ```sh
  sudo mkdir -p /opt/k8s/cert && cd /opt/k8s
  wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -e use_proxy=yes -e http_proxy=192.168.253.1:1080
  mv cfssl_linux-amd64 /opt/k8s/bin/cfssl
  
  wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -e use_proxy=yes -e http_proxy=192.168.253.1:1080
  mv cfssljson_linux-amd64 /opt/k8s/bin/cfssljson
  
  wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -e use_proxy=yes -e http_proxy=192.168.253.1:1080
  mv cfssl-certinfo_linux-amd64 /opt/k8s/bin/cfssl-certinfo
  
  chmod +x /opt/k8s/bin/*
  export PATH=/opt/k8s/bin:$PATH
  ```

- 创建配置文件

  ```sh
  cd /opt/k8s/work
  cat > ca-config.json <<EOF
  {
    "signing": {
      "default": {
        "expiry": "87600h"
      },
      "profiles": {
        "kubernetes": {
          "usages": [
              "signing",
              "key encipherment",
              "server auth",
              "client auth"
          ],
          "expiry": "87600h"
        }
      }
    }
  }
  EOF
  ```

- 证书面名请求文件：

  ```sh
  cd /opt/k8s/work
  cat > ca-csr.json <<EOF
  {
    "CN": "kubernetes",
    "key": {
      "algo": "rsa",
      "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "ST": "Shanghai",
        "L": "Shanghai",
        "O": "k8s",
        "OU": "Patac"
      }
    ]
  }
  EOF
  ```

- 生成

  ```
  cd /opt/k8s/work
  cfssl gencert -initca ca-csr.json | cfssljson -bare ca
  ls ca*
  ```

- 下发证书：

  ```
  export NODE_IPS=(192.168.253.140 192.168.253.139 192.168.253.141)
  ```

  ```sh
  cd /opt/k8s/work
  source /opt/k8s/bin/environment.sh
  for node_ip in ${NODE_IPS[@]}
    do
      echo ">>> ${node_ip}"
      ssh root@${node_ip} "mkdir -p /etc/kubernetes/cert"
      scp ca*.pem ca-config.json root@${node_ip}:/etc/kubernetes/cert
    done
  ```

### 安装kubectl

- 下载二进制文件：

  ```sh
  cd /opt/k8s/work
  wget https://dl.k8s.io/v1.14.2/kubernetes-client-linux-amd64.tar.gz -e use_proxy=yes -e http_proxy=192.168.253.1:1080
  tar -xzvf kubernetes-client-linux-amd64.tar.gz
  ```

- 远程安装文件

- 创建admin证书

  ```
  cd /opt/k8s/work
  cat > admin-csr.json <<EOF
  {
    "CN": "admin",
    "hosts": [],
    "key": {
      "algo": "rsa",
      "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "ST": "Shanghai",
        "L": "Shanghai",
        "O": "system:masters",
        "OU": "Patac"
      }
    ]
  }
  EOF
  ```

- 创建kubeconfig文件

- 分发配置文件

## etcd

etcd 是基于 Raft 的分布式 key-value 存储系统，由 CoreOS 开发，常用于服务发现、共享配置以及并发控制（如 leader 选举、分布式锁等）。kubernetes 使用 etcd 存储所有运行数据。

本文档介绍部署一个三节点高可用 etcd 集群的步骤：

- 下载和分发 etcd 二进制文件；
- 创建 etcd 集群各节点的 x509 证书，用于加密客户端(如 etcdctl) 与 etcd 集群、etcd 集群之间的数据流；
- 创建 etcd 的 systemd unit 文件，配置服务参数；
- 检查集群工作状态；

### 创建 etcd 证书和私钥

```bash
cd /opt/k8s/work
cat > etcd-csr.json <<EOF
{
  "CN": "etcd",
  "hosts": [
    "127.0.0.1",
    "192.168.253.140",
    "192.168.253.139",
    "192.168.253.141"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Shanghai",
      "L": "Shanghai",
      "O": "system:masters",
      "OU": "Patac"
    }
  ]
}
EOF
```

然后生成它的证书&派发。

### 创建etcd的systemd的unit文件(service文件)

# 部署 flannel 网络

kubernetes 要求集群内各节点(包括 master 节点)能通过 Pod 网段互联互通。flannel 使用 vxlan 技术为各节点创建一个可以互通的 Pod 网络，使用的端口为 UDP 8472（**需要开放该端口**，如公有云 AWS 等）。

flanneld 第一次启动时，从 etcd 获取配置的 Pod 网段信息，为本节点分配一个未使用的地址段，然后创建 `flannedl.1` 网络接口（也可能是其它名称，如 flannel1 等）。

flannel 将分配给自己的 Pod 网段信息写入 `/run/flannel/docker` 文件，docker 后续使用这个文件中的环境变量设置 `docker0` 网桥，从而从这个地址段为本节点的所有 Pod 容器分配 IP。

<!--简单来说，就是把集群交互使用一个插件给包起来，这样就不用弄很多接口了的意思？-->

- 创建 flannel 证书和私钥

  ```bash
  cd /opt/k8s/work
  cat > flanneld-csr.json <<EOF
  {
    "CN": "flanneld",
    "hosts": [],
    "key": {
      "algo": "rsa",
      "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "ST": "Shanghai",
        "L": "Shanghai",
        "O": "k8s",
        "OU": "Patac"
      }
    ]
  }
  EOF
  ```

- 创建 flannel 证书和私钥

- 向 etcd 写入集群 Pod 网段信息

- 创建flanneld的配置文件

- 分发 flanneld systemd unit 文件到所有节点

- 启动 flanneld 服务

  > 这里有个小坑，这里需要使用`$IFACE`参数，用于指定使用某个网口，虚拟机这里使用的是etch33.so，需要改一改。诶，还是不行。。。好吧，虚拟机互动使用的是virbr0:
  >
  > ```bash
  > cd /opt/k8s/work
  > source /opt/k8s/bin/environment.sh
  > cat > flanneld.service << EOF
  > [Unit]
  > Description=Flanneld overlay address etcd agent
  > After=network.target
  > After=network-online.target
  > Wants=network-online.target
  > After=etcd.service
  > Before=docker.service
  > 
  > [Service]
  > Type=notify
  > ExecStart=/opt/k8s/bin/flanneld \\
  >   -etcd-cafile=/etc/kubernetes/cert/ca.pem \\
  >   -etcd-certfile=/etc/flanneld/cert/flanneld.pem \\
  >   -etcd-keyfile=/etc/flanneld/cert/flanneld-key.pem \\
  >   -etcd-endpoints=${ETCD_ENDPOINTS} \\
  >   -etcd-prefix=${FLANNEL_ETCD_PREFIX} \\
  >   #-iface=${IFACE} \\
  >   -ip-masq
  > ExecStartPost=/opt/k8s/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
  > Restart=always
  > RestartSec=5
  > StartLimitInterval=0
  > 
  > [Install]
  > WantedBy=multi-user.target
  > RequiredBy=docker.service
  > EOF
  > ```

# kube-apiserver 高可用之 nginx 代理

本文档讲解使用 nginx 4 层透明代理功能实现 K8S 节点( master 节点和 worker 节点)高可用访问 kube-apiserver 的步骤。

> kube-apiserver 是 Kubernetes 最重要的核心组件之一，主要提供以下的功能
>
> - 提供集群管理的 REST API 接口，包括认证授权、数据校验以及集群状态变更等
> - 提供其他模块之间的数据交互和通信的枢纽（其他模块通过 API Server 查询或修改数据，只有 API Server 才直接操作 etcd）

### 基于 nginx 代理的 kube-apiserver 高可用方案

- 控制节点的 kube-controller-manager、kube-scheduler 是多实例部署，所以只要有一个实例正常，就可以保证高可用；
- 集群内的 Pod 使用 K8S 服务域名 kubernetes 访问 kube-apiserver， kube-dns 会自动解析出多个 kube-apiserver 节点的 IP，所以也是高可用的；
- 在每个节点起一个 nginx 进程，后端对接多个 apiserver 实例，nginx 对它们做健康检查和负载均衡；
- kubelet、kube-proxy、controller-manager、scheduler 通过本地的 nginx（监听 127.0.0.1）访问 kube-apiserver，从而实现 kube-apiserver 的高可用；

### 实际操作

- 下载和编译 nginx

- 验证编译的 nginx

- 安装和部署 nginx

- 配置4层转发

  ```bash
  cd /opt/k8s/work
  cat > kube-nginx.conf <<EOF
  worker_processes 1;
  
  events {
      worker_connections  1024;
  }
  
  stream {
      upstream backend {
          hash $remote_addr consistent;
          server 192.168.253.140:6443        max_fails=3 fail_timeout=30s;
          server 192.168.253.139:6443        max_fails=3 fail_timeout=30s;
          server 192.168.253.141:6443        max_fails=3 fail_timeout=30s;
      }
  
      server {
          listen 127.0.0.1:8443;
          proxy_connect_timeout 1s;
          proxy_pass backend;
      }
  }
  EOF
  ```

- 检查 kube-nginx 服务运行状态

## 部署master节点

kubernetes master 节点运行如下组件：

- kube-apiserver
- kube-scheduler
- kube-controller-manager
- kube-nginx

kube-apiserver、kube-scheduler 和 kube-controller-manager 均以多实例模式运行：

1. kube-scheduler 和 kube-controller-manager 会自动选举产生一个 leader 实例，其它实例处于阻塞模式，当 leader 挂了后，重新选举产生新的 leader，从而保证服务可用性；
2. kube-apiserver 是无状态的，需要通过 kube-nginx 进行代理访问，从而保证服务可用性；

<!--不行不行，这玩意太复杂了，先暂停下，这几个环境就这样保留。虽然这个有很大的帮助，但暂时不研究这个了（累死了啊啊啊啊），研究下kubeadm吧。-->

书签：[06-2.apiserver集群.md](https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/06-2.apiserver集群.md)

# kubeadm进行搭建

这里有对于环境的要求，all都要求2GB内存，对于控制要求2个cpu。

<!--这里更换了阿里云-->

1. 安装kubelet kubeadm kubectl

   ```sh
   cat <<EOF > /etc/yum.repos.d/kubernetes.repo
   [kubernetes]
   name=Kubernetes
   #baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
   baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
   enabled=1
   gpgcheck=1
   repo_gpgcheck=1
   #gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
   gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
   exclude=kube*
   EOF
   
   # Set SELinux in permissive mode (effectively disabling it)
   setenforce 0
   sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
   
   #yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
   mkdir -pv /opt/k8s/work & cd /opt/k8s/work
   yum install -y --downloadonly --downloaddir='.' kubelet kubeadm kubectl --disableexcludes=kubernetes
   yum install -y *
   
   systemctl enable --now kubelet
   systemctl enable kubelet.service
   ```

2. 安装docker

   ```
   yum install -y docker-ce
   systemctl enable docker.service
    systemctl restart docker
   ```

   粗暴点可以这届这么装。

3. docker代理

   ```
   mkdir -pv /etc/systemd/system/docker.service.d/
   cat >> /etc/systemd/system/docker.service.d/http-proxy.conf << EOF
   [Service]
   Environment="HTTP_PROXY=192.168.253.1:1080" "HTTPS_PROXY=192.168.253.1:1080" "NO_PROXY=localhost,192.168.253.0/16"
   EOF
   systemctl daemon-reload
   systemctl restart docker
   ```

   

4. docker防火墙

   Docker从1.13版本开始调整了默认的防火墙规则，禁用了iptables filter表中FOWARD链，这样会引起Kubernetes集群中跨Node的Pod无法通信，在各个Docker节点执行下面的命令：

   ```bash
   iptables -P FORWARD ACCEPT
   ```

5. 重启kubelet

   ```bash
   systemctl daemon-reload
   systemctl restart kubelet
   ```

6. 关闭 swap 分区

   如果开启了 swap 分区，kubelet 会启动失败(可以通过将参数 --fail-swap-on 设置为 false 来忽略 swap on)，故需要在每台机器上关闭 swap 分区。同时注释 `/etc/fstab` 中相应的条目，防止开机自动挂载 swap 分区：

   ```bash
   swapoff -a
   sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab 
   ```

7. 关闭防火墙

   ```
   systemctl stop firewalld
   systemctl disable firewalld
   ```

8. 关闭seliunx(第一步做了，需要要重启下)

   ```bash
   setenforce 0
   ```

   ```bash
   vi /etc/selinux/config
   SELINUX=disabled
   ```

   查看：

   ```bash
   getenforce
   ```

9. 对k8s设定流量

   ```bash
   cat <<EOF > /etc/sysctl.d/k8s.conf
   net.bridge.bridge-nf-call-ip6tables = 1
   net.bridge.bridge-nf-call-iptables = 1
   EOF
   ```

10. 需要设置`hostname`：

   报错：

   ```
   name: Invalid value: ".": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')
   ```

   这里是hostname命名不符合这个规范的原因，[这个issue中有提到](https://github.com/kubernetes/kubeadm/issues/347#issuecomment-318757572)。

   修改hostname

   ```
   hostnamectl set-hostname k8s-adm-1
   ```

   并设置对应hostname的路由表：

   ```
   cat >> /etc/hosts <<EOF
   192.168.253.142 k8s-adm-1
   192.168.253.145 k8s-adm-2
   EOF
   ```

11. 启动kubeadm

    ```bash
    kubeadm reset -f
    rm -rf $HOME/.kube/config
    
    
    kubeadm init --apiserver-advertise-address '0.0.0.0'
    ```

    > 代理
    >
    > ```bash
    > cat >> /etc/profile <<EOF
    > export HTTP_PROXY="192.168.253.1:1080"
    > export HTTPS_PROXY="192.168.253.1:1080"
    > export NO_PROXY="localhost,192.168.253.0/16"
    > EOF
    > unset HTTP_PROXY HTTPS_PROXY NO_PROXY
    > ```

    找到一个指导：<https://yanyixing.github.io/2018/12/08/install-k8s/>

    然后kubeadm会帮你检查很多东西：

    ```
    	[WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
    	[WARNING HTTPProxy]: Connection to "https://192.168.253.142" uses proxy "http://192.168.253.1:1080". If that is not intended, adjust your proxy settings
    	[WARNING HTTPProxyCIDR]: connection to "10.96.0.0/12" uses proxy "http://192.168.253.1:1080". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration
    	[WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service'
    	[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
    	[WARNING Hostname]: hostname "k8s-adm-1" could not be reached
    	[WARNING Hostname]: hostname "k8s-adm-1": lookup k8s-adm-1 on 192.168.253.2:53: server misbehaving
    ```

    > ```
    > hostname "k8s-adm-1" could not be reache
    > ```
    >
    > <https://stackoverflow.com/questions/45391667/kubeadm-v1-7-2-hostname-could-not-be-reached>
    >
    > 说明没有把这玩意写在hosts里面：
    >
    > ```bash
    > cat >> /etc/hosts <<EOF
    > 192.168.253.142 k8s-adm-1
    > EOF
    > ```

    > ```
    > dial tcp 127.0.0.1:10248: connect: connection refused.
    > ```
    >
    > <https://stackoverflow.com/questions/55179471/cannot-use-kubeadm-init-single-kubernetes-cluster>
    >
    > 将流量转发给iptable
    >
    > ```bash
    > sysctl net.bridge.bridge-nf-call-iptables=1
    > ```
    >
    > ```bash
    > kubeadm init --pod-network-cidr=192.168.253.0/24  --token-ttl=0
    > ```
    >
    > 设定pod的网段。---没用，我没有使用pod网络
    >
    > ```bash
    > kubeadm init --token-ttl=0
    > ```
    >
    > 这个是设定不过期的令牌，没用啊。

    > <https://github.com/kubernetes/kubernetes/issues/53333>
    >
    > 这个倒是很像，但不是啊

    > 我日，这里发现是没有启动etcd？
    >
    > https://github.com/kubernetes/kubeadm/issues/1082#issuecomment-417551023
    >
    > 但我这里etcd的容器是可以正常启动啊
    >
    > 关于--cri-socket string     缺省值: "/var/run/dockershim.sock"

    实际上是kuberlet有问题：

    ```bash
    failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: "cgroupfs" is different from docker cgroup driver: "systemd”
    ```

12. 终于特么启动起来了

    ```bash
    kubeadm join 192.168.253.142:6443 --token 88pzvl.fgtowjmq0e30qe26 \
        --discovery-token-ca-cert-hash sha256:5554f73afc415496991c4abc8e8ae75183832cd18886f49023df096c2f2a9fd9
    ```

    好吧，前面到哪了？

13. `kubectl get nodes`

    要使`kubectl`为非root用户工作，请运行以下命令，这些命令也是`kubeadm init`输出的一部分：

    ```bash
    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config
    ```

    或者，如果您是`root`用户，则可以运行：

    ```bash
    export KUBECONFIG=/etc/kubernetes/admin.conf
    ```

14. dashboard

    ```bash
    kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml
    ```

    然后可以在：

    ```
    kubectl proxy --address 0.0.0.0 --accept-hosts '.*'
    ```

    ```
    http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/
    http://192.168.253.142:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/
    ```

    <!--还没用起来就得先重启一次。-->

    [删除dashboard:](https://stackoverflow.com/questions/46173307/how-do-i-remove-the-kubernetes-dashboard-pod-from-my-deployment-on-google-cloud)

    先看需要删哪些：

    ```bash
    kubectl get secret,sa,role,rolebinding,services,deployments --namespace=kube-system | grep dashboard
    ```

    然后一个个删掉：

    ```bash
    kubectl delete deployment kubernetes-dashboard --namespace=kube-system 
    kubectl delete service kubernetes-dashboard  --namespace=kube-system 
    kubectl delete role kubernetes-dashboard-minimal --namespace=kube-system 
    kubectl delete rolebinding kubernetes-dashboard-minimal --namespace=kube-system
    kubectl delete sa kubernetes-dashboard --namespace=kube-system 
    kubectl delete secret kubernetes-dashboard-certs --namespace=kube-system
    kubectl delete secret kubernetes-dashboard-key-holder --namespace=kube-system
    ```

    。。。

    还是访问不了。。

    > <https://github.com/kubernetes/dashboard/issues/2986>
    >
    > 这里貌似也是这个问题。
    >
    > 上面说：
    >
    > > [@Angha](https://github.com/Angha) Kubernetes v1.13删除了Heapster。短期内，您仍然可以安装Heapster以使工作正常。
    > >
    > > 从长远来看，已经有一个飞行中的修复（[＃3504](https://github.com/kubernetes/dashboard/pull/3504)）将在下一个仪表板版本中。
    >
    > all right：
    >
    > | Kubernetes version | 1.8  | 1.9  | 1.10 | 1.11 | 1.12 | 1.13 |
    > | ------------------ | ---- | ---- | ---- | ---- | ---- | ---- |
    > | Compatibility      | ✓    | ✓    | ✓    | ?    | ?    | ✕    |
    >
    > 暂停这个的继续开发。

15. 另外还有两个coredns也处于pending状态。<!--你两是干嘛的，为啥不启动啊-->

    理论上应该能看到，但我没看到。然后查看原因：

    ```bash
    # 查看pod状态
    # 一般应用
    kubectl get pods
    # 系统级应用
    kubectl get pods --namespace kube-system
    ```

    这里发现我们的dashborad处于`pending`状态：

    ```bash
    [root@k8s-adm-1 ~]# kubectl get pods --namespace kube-system
    NAME                                    READY   STATUS    RESTARTS   AGE
    coredns-5c98db65d4-8dsht                0/1     Pending   0          131m
    coredns-5c98db65d4-r4dgq                0/1     Pending   0          131m
    etcd-k8s-adm-1                          1/1     Running   0          130m
    kube-apiserver-k8s-adm-1                1/1     Running   0          130m
    kube-controller-manager-k8s-adm-1       1/1     Running   0          130m
    kube-proxy-pplgc                        1/1     Running   0          131m
    kube-scheduler-k8s-adm-1                1/1     Running   0          130m
    kubernetes-dashboard-7d75c474bb-8wf45   0/1     Pending   0          15m
    ```

    <https://github.com/kubernetes/kubeadm/issues/980>

    这里建议安装网络插件？：

    这里使用的是wave-net

    <https://www.weave.works/blog/weave-net-kubernetes-integration/>

    ```
    kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
    ```

    然后查看pods状态：

    ```bash
    kubectl describe pods kubernetes-dashboard-7d75c474bb-8wf45  --namespace kube-system
    ```

    注意，普通的不需要后面的namespace。。。。吧。

    这里显示：

    ```bash
    Events:
      Type     Reason            Age                 From               Message
      ----     ------            ----                ----               -------
      Warning  FailedScheduling  75s (x13 over 18m)  default-scheduler  0/1 nodes are available: 1 node(s) had taints that the pod didn't tolerate.
    ```

    > 官方的解决方法：
    >
    > ### 我的Pod保持Pending
    >
    > 如果一个pod被卡在`Pending`中，就意味着它不能调度在某个节点上。一般来说，这是因为某种类型的资源不足 阻止调度。 看看上面的命令`kubectl describe ...`的输出。调度器的消息中应该会包含无法调度Pod的原因。 理由包括：
    >
    > #### 资源不足
    >
    > 您可能已经耗尽了集群中供应的CPU或内存。在这个情况下你可以尝试几件事情：
    >
    > - [添加更多节点](https://kubernetes.io/docs/admin/cluster-management/#resizing-a-cluster) 到集群。
    >
    > - [终止不需要的pod](https://kubernetes.io/docs/user-guide/pods/single-container/#deleting_a_pod) 为pending中的pods提供空间。
    >
    > - 检查该pod是否不大于您的节点。例如，如果全部节点具有`cpu:1`容量，那么具有`cpu: 1.1`请求的pod永远不会被调度。
    >
    >   您可以使用`kubectl get nodes -o <format>`命令来检查节点容量。 下面是一些能够提取必要信息的命令示例：
    >
    >   kubectl get nodes -o yaml | grep ‘\sname|cpu|memory’ kubectl get nodes -o json | jq ‘.items[] | {name: .metadata.name, cap: .status.capacity}’
    >
    > 可以考虑配置[资源配额](https://kubernetes.io/docs/concepts/policy/resource-quotas/)来限制可耗用的资源总量。如果与命名空间一起使用，它可以防止一个团队吞噬所有的资源。
    >
    > #### 使用hostPort
    >
    > 当你将一个pod绑定到一个`hostPort`时，这个pod能被调度的位置数量有限。 在大多数情况下，`hostPort`是不必要的; 尝试使用服务对象来暴露您的pod。 如果你需要`hostPort`，那么你可以调度的Pod数量不能超过集群的节点个数。



## 增加一个新的节点

这次是安装一个作为普通端的节点。

部件安装：

同上的1~9步。

当然，这次可以直接用之前的那个包。

在解决掉基本的一些docker等问题后：

运行11里面的join即可。

但这里

```bash
[root@localhost work]# kubectl get nodes
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

这个的解决方式是：

```bash
export KUBECONFIG=/etc/kubernetes/kubelet.conf
```

主机上也显示：

```bash
[root@k8s-adm-1 manifests]# kubectl get nodes
NAME        STATUS     ROLES    AGE    VERSION
k8s-adm-1   Ready      master   22h    v1.15.0
k8s-adm-2   NotReady   <none>   2m6s   v1.15.0
```

这个解决方式是，加入对应的网络插件：

```

```

。。。先研究下，之前aploy的是什么玩意：

是一个叫做wave-net的玩意，理论上只需要主节点搞定，支节点也有了啊

查看：

```
# kubectl get pods --namespace=kube-system
weave-net-ttgp8                     0/2     ContainerCreating   0          4m39s
weave-net-vswhq                     2/2     Running             0          41h
```

有个weave-net没启动起来。

查看详情：

```
# kubectl describe pod weave-net-ttgp8  --namespace=kube-system
Name:           weave-net-ttgp8
Namespace:      kube-system
Priority:       0
Node:           k8s-adm-2/192.168.253.145
..........
Events:
  Type     Reason                  Age                 From                Message
  ----     ------                  ----                ----                -------
  Normal   Scheduled               5m21s               default-scheduler   Successfully assigned kube-system/weave-net-ttgp8 to k8s-adm-2
  Warning  FailedCreatePodSandBox  1s (x8 over 4m44s)  kubelet, k8s-adm-2  Failed create pod sandbox: rpc error: code = Unknown desc = failed pulling image "k8s.gcr.io/pause:3.1": Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
```

发现是没有pull对应的镜像下来。

因为docker没有设置好对应的代理。然后参考上面的设置代理的方式，来一遍。



